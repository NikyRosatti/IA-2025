# Ensemble Learning üìä‚ú®

**Ensemble Learning** combina m√∫ltiples modelos para mejorar la **precisi√≥n, estabilidad y robustez** de las predicciones. En lugar de depender de un solo modelo, usamos la fuerza de varios modelos trabajando juntos.  

---

## Voting üó≥Ô∏è

**Voting** combina diferentes modelos y decide la predicci√≥n final bas√°ndose en la **votaci√≥n de sus resultados**.  

### Tipos de Voting

- **Hard Voting (votaci√≥n dura)**  
  - Cada modelo emite su predicci√≥n como **clase discreta**.  
  - La clase que obtiene la **mayor√≠a de votos** es la predicci√≥n final.  
  - Ejemplo:
    ```
    Modelo A: 0
    Modelo B: 1
    Modelo C: 1
    ‚Üí Resultado final: 1
    ```

- **Soft Voting (votaci√≥n suave)**  
  - Cada modelo predice **probabilidades** para cada clase.  
  - Se suman o promedian las probabilidades.  
  - La clase con la **mayor probabilidad promedio** se elige como resultado final.  

- **Weighted Voting** üèãÔ∏è‚Äç‚ôÇÔ∏è  
  - Se pueden asignar pesos a los modelos para que algunos tengan m√°s influencia en la votaci√≥n.

---

## Bagging üé≤

**Bagging (Bootstrap Aggregating)** crea m√∫ltiples versiones de un modelo usando **subconjuntos de datos diferentes**, entrenando un modelo en cada subconjunto y combinando sus predicciones mediante voting.  

- √ötil cuando los datos son **ruidosos o peque√±os**.  
- Ejemplo: **Random Forest**
  - Conjunto de **Decision Trees**.  
  - Cada √°rbol se entrena con una **muestra aleatoria de los datos**.  
  - La predicci√≥n final se obtiene por **votaci√≥n**.  
  - Introduce **aleatoriedad** para mejorar la generalizaci√≥n.

---

## Boosting ‚ö°

**Boosting** crea modelos secuenciales, donde cada nuevo modelo aprende **de los errores de los anteriores**.  

- **Enfoque:** dar m√°s peso a los datos que los modelos anteriores clasificaron incorrectamente.  
- **Utilidad:** mejora la **precisi√≥n**, ideal para problemas donde los errores son costosos.  
- Ejemplos: **AdaBoost, Gradient Boosting, XGBoost**.

---

## Stacking üíä

**Stacking** (apilamiento de modelos) combina varios modelos base entrenados de manera independiente y utiliza un **metamodelo** para aprender de sus predicciones.  

- A diferencia de Boosting, **no ajusta los modelos secuencialmente**, sino que todos los modelos base se entrenan primero y sus predicciones sirven como **entrada para un nuevo modelo (metamodelo)**.  
- El **metamodelo** aprende a combinar las salidas de los modelos base para obtener una predicci√≥n m√°s precisa.  
- **Utilidad:**
    - Mezclar modelos distintos (por ejemplo, √°rboles de decisi√≥n + regresi√≥n log√≠stica + SVM).  
    - Capturar patrones complejos que un solo modelo no podr√≠a detectar.  
    - Mejorar precisi√≥n y generalizaci√≥n en problemas complejos de clasificaci√≥n o regresi√≥n.

> Usa crossover para entrenar.

---

## Resumen üé®

| M√©todo       | C√≥mo funciona                                         | Uso principal                                  |
|-------------|------------------------------------------------------|-----------------------------------------------|
| Hard Voting  | Predicciones discretas, mayor√≠a de votos            | Clasificaci√≥n robusta                          |
| Soft Voting  | Promedio de probabilidades                            | Cuando los modelos predicen probabilidades confiables |
| Bagging      | Modelos independientes sobre muestras aleatorias    | Reducir varianza y overfitting                |
| Boosting     | Modelos secuenciales corrigiendo errores anteriores | Reducir bias, aumentar precisi√≥n              |
| Stacking     | Modelos base + metamodelo que aprende de sus salidas| Mezclar modelos y capturar patrones complejos |
